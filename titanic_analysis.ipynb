{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will perform an analysis of the Titanic dataset. The goal is to build predictive models to determine the survival of passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "In this section, we load the datasets from the `train.csv` and `test.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "In this section, we perform an initial exploration of the training dataset to understand its structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Overview\n",
    "print(train_df.head())\n",
    "print(train_df.info())\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, we perform exploratory data analysis to visualize and understand the relationships between different features and the target variable 'Survived'. This helps us gain insights into the data and identify patterns that may be useful for building predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "sns.pairplot(train_df, hue='Survived')\n",
    "plt.show()\n",
    "sns.heatmap(train_df.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Missing Values\n",
    "\n",
    "We handle missing values in the dataset to ensure completeness:\n",
    "- For the 'Age' column, we fill missing values with the median age.\n",
    "- For the 'Embarked' column, we use the most common embarkation point (mode).\n",
    "- In the test dataset, we also fill missing values in the 'Fare' column with the median fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "train_df['Age'].fillna(train_df['Age'].median(), inplace=True)\n",
    "train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)\n",
    "test_df['Age'].fillna(test_df['Age'].median(), inplace=True)\n",
    "test_df['Fare'].fillna(test_df['Fare'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Categorical Variables\n",
    "\n",
    "We use `LabelEncoder` to convert the categorical variables 'Sex' and 'Embarked' into numerical format. This step is necessary for machine learning algorithms to process the data. \n",
    "\n",
    "- **LabelEncoder**: Assigns a unique integer to each category (e.g., 'male' as 1, 'female' as 0).\n",
    "\n",
    "The encoder is applied to both the training and test datasets for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Sex'] = label_encoder.fit_transform(train_df['Sex'])\n",
    "train_df['Embarked'] = label_encoder.fit_transform(train_df['Embarked'])\n",
    "test_df['Sex'] = label_encoder.transform(test_df['Sex'])\n",
    "test_df['Embarked'] = label_encoder.transform(test_df['Embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Numerical Features\n",
    "\n",
    "We use `StandardScaler` to normalize the numerical features 'Age' and 'Fare'. Normalization scales the features to have a mean of 0 and a standard deviation of 1, which helps improve the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "train_df[['Age', 'Fare']] = scaler.fit_transform(train_df[['Age', 'Fare']])\n",
    "test_df[['Age', 'Fare']] = scaler.transform(test_df[['Age', 'Fare']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We create a new feature 'FamilySize' by combining 'SibSp' and 'Parch' to represent the total number of family members aboard the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\n",
    "test_df['FamilySize'] = test_df['SibSp'] + test_df['Parch'] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "In this section, we create visualizations to explore the relationships between different features and the target variable 'Survived'. These visualizations help us gain insights into the data and identify patterns that may be useful for building predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "sns.countplot(x='Survived', data=train_df)\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x='Pclass', hue='Survived', data=train_df)\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x='Sex', hue='Survived', data=train_df)\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(train_df['Age'], bins=30, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "We define the feature matrix `X` and target variable `y`, split the data into training and validation sets, and initialize a `RandomForestClassifier` model. The model is then trained on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n",
    "X = train_df.drop(['Survived', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "y = train_df['Survived']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(model, 'titanic_model.pkl')\n",
    "print(\"Model saved to 'titanic_model.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "We analyze which features were most important in predicting survival, using the importance scores from our Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "We use GridSearchCV to systematically search for the optimal combination of hyperparameters for our RandomForest model. This involves testing different values for parameters like the number of trees (n_estimators), maximum depth, minimum samples required to split a node, and minimum samples required at a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Use the best model\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "We use cross-validation to evaluate the model's performance more robustly. Cross-validation helps in assessing how well the model generalizes to an independent dataset by splitting the data into multiple folds and training the model on each fold. This process provides a more reliable estimate of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation\n",
    "cv_scores = cross_val_score(model, X, y)\n",
    "print(f'Cross-validation scores: {cv_scores}')\n",
    "print(f'Mean cross-validation score: {cv_scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We evaluate the model's performance using accuracy score, confusion matrix, and classification report. These metrics help us understand how well the model is performing in predicting the survival of passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "y_pred = model.predict(X_val)\n",
    "print(f'Accuracy: {accuracy_score(y_val, y_pred)}')\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, we built a machine learning model to predict the survival of passengers on the Titanic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
